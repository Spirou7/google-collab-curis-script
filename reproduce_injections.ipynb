{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from local_tpu_resolver import LocalTPUClusterResolver\n",
    "\n",
    "from models.resnet import resnet_18\n",
    "from models.backward_resnet import backward_resnet_18\n",
    "from models.resnet_nobn import resnet_18_nobn\n",
    "from models.backward_resnet_nobn import backward_resnet_18_nobn\n",
    "from models import efficientnet\n",
    "from models import backward_efficientnet\n",
    "from models import densenet\n",
    "from models import backward_densenet\n",
    "from models import nf_resnet\n",
    "from models import backward_nf_resnet\n",
    "\n",
    "import config\n",
    "from prepare_data import generate_datasets\n",
    "import math\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from models.inject_utils import *\n",
    "from injection import read_injection\n",
    "\n",
    "\n",
    "tf.config.set_soft_device_placement(True)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "golden_grad_idx = {\n",
    "    'resnet18': -2,\n",
    "    'resnet18_nobn': -2,\n",
    "    'resnet18_sgd': -2,\n",
    "    'effnet': -4,\n",
    "    'densenet': -2,\n",
    "    'nfnet': -2\n",
    "    }\n",
    "\n",
    "class Replay():\n",
    "    model = ''\n",
    "    stage = ''\n",
    "    fmodel = ''\n",
    "    target_worker = -1\n",
    "    target_layer = ''\n",
    "    target_epoch = -1\n",
    "    target_step =  -1\n",
    "    inj_pos = []\n",
    "    inj_values = []\n",
    "    seed = 123\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    desc = \"Tensorflow implementation of Resnet\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "    parser.add_argument('--file', type=str, help=\"Choose a csv file to replay\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def get_model(m_name, seed):\n",
    "    if m_name == 'resnet18' or m_name == 'resnet18_sgd':\n",
    "        model = resnet_18(seed, m_name)\n",
    "        model.build(input_shape=(None, config.image_height, config.image_width, config.channels))\n",
    "        back_model = backward_resnet_18(m_name)\n",
    "\n",
    "    elif m_name == 'resnet18_nobn':\n",
    "        model = resnet_18_nobn(seed)\n",
    "        model.build(input_shape=(None, config.image_height, config.image_width, config.channels))\n",
    "        back_model = backward_resnet_18_nobn()\n",
    "\n",
    "    elif m_name == 'effnet':\n",
    "        model = efficientnet.efficient_net_b0(seed)\n",
    "        model.build(input_shape=(None, config.image_height, config.image_width, config.channels))\n",
    "        back_model = backward_efficientnet.backward_efficient_net_b0()\n",
    "\n",
    "    elif m_name == 'densenet':\n",
    "        model = densenet.densenet_121(seed)\n",
    "        model.build(input_shape=(None, config.image_height, config.image_width, config.channels))\n",
    "        back_model = backward_densenet.backward_densenet_121()\n",
    "\n",
    "    elif m_name == 'nfnet':\n",
    "        model = nf_resnet.NF_ResNet(num_classes=10, seed=seed, alpha=1, stochdepth_rate=0)\n",
    "        model.build(input_shape=(None, config.image_height, config.image_width, config.channels))\n",
    "        back_model = backward_nf_resnet.BackwardNF_ResNet(num_classes=10, alpha=1, stochdepth_rate=0)\n",
    "\n",
    "    return model, back_model\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    if args is None:\n",
    "        exit()\n",
    "\n",
    "    # TPU settings\n",
    "    tpu_name = os.getenv('TPU_NAME')\n",
    "    resolver = LocalTPUClusterResolver()\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    per_replica_batch_size = config.BATCH_SIZE // strategy.num_replicas_in_sync\n",
    "    print(\"Finish TPU strategy setting!\")\n",
    "\n",
    "\n",
    "    rp = read_injection(args.file)\n",
    "    #rp.seed = 123\n",
    "\n",
    "    # get the dataset\n",
    "    train_dataset, valid_dataset, train_count, valid_count = generate_datasets(rp.seed)\n",
    "\n",
    "    train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "    valid_dataset = strategy.experimental_distribute_dataset(valid_dataset)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model, back_model = get_model(rp.model, rp.seed)\n",
    "\t# define loss and optimizer\n",
    "        if 'sgd' in rp.model:\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "            \tinitial_learning_rate=rp.learning_rate,\n",
    "            \tdecay_steps = 2000,\n",
    "            \tend_learning_rate=0.001)\n",
    "            model.optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "        elif 'effnet' in rp.model:\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=rp.learning_rate,\n",
    "                decay_steps = 2000,\n",
    "                end_learning_rate=0.0005)\n",
    "            model.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        else:\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "                initial_learning_rate=rp.learning_rate,\n",
    "                decay_steps = 5000,\n",
    "                end_learning_rate=0.0001)\n",
    "            model.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "        valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "        valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(iterator):\n",
    "        def step_fn(inputs):\n",
    "            images, labels = inputs\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs, _, _, l_outputs = model(images, training=True, inject=False)\n",
    "                predictions = outputs['logits']\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "                avg_loss = tf.nn.compute_average_loss(loss, global_batch_size=config.BATCH_SIZE)\n",
    "\n",
    "            tvars = model.trainable_variables\n",
    "            gradients = tape.gradient(avg_loss, tvars)\n",
    "            model.optimizer.apply_gradients(grads_and_vars=list(zip(gradients, tvars)))\n",
    "\n",
    "            train_loss.update_state(avg_loss * strategy.num_replicas_in_sync)\n",
    "            train_accuracy.update_state(labels, predictions)\n",
    "            return avg_loss\n",
    "\n",
    "        return strategy.run(step_fn, args=(next(iterator),))\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def fwrd_inj_train_step1(iter_inputs, inj_layer):\n",
    "        def step1_fn(inputs):\n",
    "            images, labels = inputs\n",
    "            outputs, l_inputs, l_kernels, l_outputs = model(images, training=True, inject=False)\n",
    "            predictions = outputs['logits']\n",
    "            return l_inputs[inj_layer], l_kernels[inj_layer], l_outputs[inj_layer]\n",
    "        return strategy.run(step1_fn, args=(iter_inputs,))\n",
    "\n",
    "    @tf.function\n",
    "    def fwrd_inj_train_step2(iter_inputs, inj_args, inj_flag):\n",
    "        def step2_fn(inputs, inject):\n",
    "            with tf.GradientTape() as tape:\n",
    "                images, labels = inputs\n",
    "                outputs, l_inputs, l_kernels, l_outputs = model(images, training=True, inject=inject, inj_args=inj_args)\n",
    "                predictions = outputs['logits']\n",
    "                grad_start = outputs['grad_start']\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "                avg_loss = tf.nn.compute_average_loss(loss, global_batch_size=config.BATCH_SIZE)\n",
    "\n",
    "            man_grad_start, golden_gradients = tape.gradient(avg_loss, [grad_start, model.trainable_variables])\n",
    "            manual_gradients, _, _, _ = back_model(man_grad_start, l_inputs, l_kernels)\n",
    "\n",
    "            gradients = manual_gradients + golden_gradients[golden_grad_idx[rp.model]:]\n",
    "            model.optimizer.apply_gradients(list(zip(gradients, model.trainable_variables)))\n",
    "\n",
    "            train_loss.update_state(avg_loss * strategy.num_replicas_in_sync)\n",
    "            train_accuracy.update_state(labels, predictions)\n",
    "            return avg_loss\n",
    "\n",
    "        return strategy.run(step2_fn, args=(iter_inputs, inj_flag))\n",
    "\n",
    "    @tf.function\n",
    "    def bkwd_inj_train_step1(iter_inputs, inj_layer):\n",
    "        def step1_fn(inputs):\n",
    "            images, labels = inputs\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs, l_inputs, l_kernels, _ = model(images, training=True, inject=False)\n",
    "                predictions = outputs['logits']\n",
    "                grad_start = outputs['grad_start']\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "                avg_loss = tf.nn.compute_average_loss(loss, global_batch_size=config.BATCH_SIZE)\n",
    "            man_grad_start = tape.gradient(avg_loss, grad_start)\n",
    "            _, bkwd_inputs, bkwd_kernels, bkwd_outputs = back_model(man_grad_start, l_inputs, l_kernels)\n",
    "            return bkwd_inputs[inj_layer], bkwd_kernels[inj_layer], bkwd_outputs[inj_layer]\n",
    "\n",
    "        return strategy.run(step1_fn, args=(iter_inputs,))\n",
    "\n",
    "    @tf.function\n",
    "    def bkwd_inj_train_step2(iter_inputs, inj_args, inj_flag):\n",
    "        def step2_fn(inputs, inject):\n",
    "            images, labels = inputs\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs, l_inputs, l_kernels, l_outputs = model(images, training=True, inject=False)\n",
    "                predictions = outputs['logits']\n",
    "                grad_start = outputs['grad_start']\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "                avg_loss = tf.nn.compute_average_loss(loss, global_batch_size=config.BATCH_SIZE)\n",
    "            man_grad_start, golden_gradients = tape.gradient(avg_loss, [grad_start, model.trainable_variables])\n",
    "            manual_gradients, _, _, _ = back_model(man_grad_start, l_inputs, l_kernels, inject=inject, inj_args=inj_args)\n",
    "\n",
    "            gradients = manual_gradients + golden_gradients[golden_grad_idx[rp.model]:]\n",
    "            model.optimizer.apply_gradients(list(zip(gradients, model.trainable_variables)))\n",
    "\n",
    "            train_loss.update_state(avg_loss * strategy.num_replicas_in_sync)\n",
    "            train_accuracy.update_state(labels, predictions)\n",
    "\n",
    "            return avg_loss\n",
    "\n",
    "        return strategy.run(step2_fn, args=(iter_inputs, inj_flag))\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def valid_step(iterator):\n",
    "        def step_fn(inputs):\n",
    "            images, labels = inputs\n",
    "            outputs , _, _, _ = model(images, training=False)\n",
    "            predictions = outputs['logits']\n",
    "            v_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "            v_loss = tf.nn.compute_average_loss(v_loss, global_batch_size=config.BATCH_SIZE)\n",
    "            valid_loss.update_state(v_loss)\n",
    "            valid_accuracy.update_state(labels, predictions)\n",
    "        return strategy.run(step_fn, args=(next(iterator),))\n",
    "\n",
    "    steps_per_epoch = math.ceil(train_count / config.BATCH_SIZE)\n",
    "    valid_steps_per_epoch = math.ceil(valid_count / config.VALID_BATCH_SIZE)\n",
    " \n",
    "    target_epoch = rp.target_epoch\n",
    "    target_step = rp.target_step\n",
    "\n",
    "    train_recorder = open(\"replay_{}.txt\".format(args.file[args.file.rfind('/')+1:args.file.rfind('.')]), 'w')\n",
    "    record(train_recorder, \"Inject to epoch: {}\\n\".format(target_epoch))\n",
    "    record(train_recorder, \"Inject to step: {}\\n\".format(target_step))\n",
    "\n",
    "    ckpt_path = os.path.join(config.golden_model_dir, rp.model, \"epoch_{}\".format(target_epoch - 1))\n",
    "    record(train_recorder, \"Load weights from {}\\n\".format(ckpt_path))\n",
    "    model.load_weights(ckpt_path)\n",
    "\n",
    "\n",
    "    start_epoch = target_epoch\n",
    "    total_epochs = config.EPOCHS\n",
    "    early_terminate = False\n",
    "    epoch = start_epoch\n",
    "    while epoch < total_epochs:\n",
    "        if early_terminate:\n",
    "            break\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        valid_accuracy.reset_states()\n",
    "        step = 0\n",
    "\n",
    "        train_iterator = iter(train_dataset)\n",
    "        for step in range(steps_per_epoch):\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            if early_terminate:\n",
    "                break\n",
    "            if epoch != target_epoch or step != target_step:\n",
    "                losses = train_step(train_iterator)\n",
    "            else:\n",
    "                iter_inputs = next(train_iterator)\n",
    "                inj_layer = rp.target_layer\n",
    "\n",
    "                if 'fwrd' in rp.stage:\n",
    "                    l_inputs, l_kernels, l_outputs = fwrd_inj_train_step1(iter_inputs, inj_layer)\n",
    "                else:\n",
    "                    l_inputs, l_kernels, l_outputs = bkwd_inj_train_step1(iter_inputs, inj_layer)\n",
    "\n",
    "                inj_args, inj_flag = get_replay_args(InjType[rp.fmodel], rp, strategy, inj_layer, l_inputs, l_kernels, l_outputs, train_recorder)\n",
    "\n",
    "                if 'fwrd' in rp.stage:\n",
    "                    losses = fwrd_inj_train_step2(iter_inputs, inj_args, inj_flag)\n",
    "                else:\n",
    "                    losses = bkwd_inj_train_step2(iter_inputs, inj_args, inj_flag)\n",
    "\n",
    "            record(train_recorder, \"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\\n\".format(epoch,\n",
    "                             total_epochs,\n",
    "                             step,\n",
    "                             steps_per_epoch,\n",
    "                             train_loss.result(),\n",
    "                             train_accuracy.result()))\n",
    "\n",
    "            if not np.isfinite(train_loss.result()):\n",
    "                record(train_recorder, \"Encounter NaN! Terminate training!\\n\")\n",
    "                early_terminate = True\n",
    "\n",
    "        if not early_terminate:\n",
    "            valid_iterator = iter(valid_dataset)\n",
    "            for _ in range(valid_steps_per_epoch):\n",
    "                valid_step(valid_iterator)\n",
    "\n",
    "            record(train_recorder, \"End of epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n",
    "                \"valid loss: {:.5f}, valid accuracy: {:.5f}\\n\".format(epoch,\n",
    "                             config.EPOCHS,\n",
    "                             train_loss.result(),\n",
    "                             train_accuracy.result(),\n",
    "                             valid_loss.result(),\n",
    "                             valid_accuracy.result()))\n",
    "\n",
    "            # NaN value in validation\n",
    "            if not np.isfinite(valid_loss.result()):\n",
    "                record(train_recorder, \"Encounter NaN! Terminate training!\\n\")\n",
    "\n",
    "                early_terminate = True\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
